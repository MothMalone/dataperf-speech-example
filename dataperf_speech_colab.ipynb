{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5uSni11iDCb"
      },
      "source": [
        "This notebook works through running the [DataPerf Speech](https://www.dataperf.org/training-set-selection-speech) challenge evaluation with a [baseline selection algorithm](https://github.com/harvard-edge/dataperf-speech-example/blob/main/selection/implementations/baseline_selection.py).\n",
        "\n",
        "We start by cloning our example selection algorithm repository and installing some additional dependencies not preinstalled in Colab environments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/harvard-edge/dataperf-speech-example/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/dataperf-speech-example/\")\n",
        "import os\n",
        "os.chdir(\"dataperf-speech-example/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSzmJSd-liEX"
      },
      "source": [
        "Next, we download the spoken word embeddings which we will use for training coreset selection and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiJKEor_fo4k"
      },
      "outputs": [],
      "source": [
        "!python utils/download_data.py --output_path workspace/data 1> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWqcH_PfH_f"
      },
      "source": [
        "Below, we generate a set of 25 training samples from the available embeddings for each language, using our default selection algorithm (which simply performs crossfold-validation). The evaluation strategy can be changed by editing `dataperf-speech-example/workspace/dataperf_speech_config.yaml` \n",
        "\n",
        "The goal of this challenge is to add your own selection algorithm and outperform the provided baselines' macro F1 scores.\n",
        "\n",
        "The selection algorithm will output a training file for each language, `en_25_train.json`, `id_25_train.json`, and `pt_25_train.json`.\n",
        "\n",
        "These are the files you would upload to Dynabench for official evaluation, but in the next cell, we will run local unofficial evaluation using our provided evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 18.12it/s]\n",
            "Loading nontargets: 100%|█████████████████████| 100/100 [00:06<00:00, 15.89it/s]\n",
            "num_targets: 15\n",
            "per_target_class_size: 3\n",
            "nontarget_class_size: 10\n",
            "Using original method to select data subset ...\n",
            "k-fold cross validation: 100%|██████████████████| 10/10 [01:42<00:00, 10.22s/it]\n",
            "final best_score=0.6630933625692349\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 50.97it/s]\n",
            "Loading nontargets: 100%|█████████████████████| 100/100 [00:01<00:00, 84.28it/s]\n",
            "num_targets: 15\n",
            "per_target_class_size: 3\n",
            "nontarget_class_size: 10\n",
            "Using original method to select data subset ...\n",
            "k-fold cross validation: 100%|██████████████████| 10/10 [00:26<00:00,  2.64s/it]\n",
            "final best_score=0.6547101271438757\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 22.94it/s]\n",
            "Loading nontargets: 100%|█████████████████████| 100/100 [00:03<00:00, 30.55it/s]\n",
            "num_targets: 15\n",
            "per_target_class_size: 3\n",
            "nontarget_class_size: 10\n",
            "Using original method to select data subset ...\n",
            "k-fold cross validation: 100%|██████████████████| 10/10 [00:57<00:00,  5.71s/it]\n",
            "final best_score=0.8138524334872996\n"
          ]
        }
      ],
      "source": [
        "TRAIN_SIZE = 25 # or 60\n",
        "for lang in [\"en\", \"id\", \"pt\"]:\n",
        "  !python -m selection.main \\\n",
        "     --language \"{lang}\" \\\n",
        "     --allowed_training_set \"workspace/data/dataperf_{lang}_data/allowed_training_set.yaml\" \\\n",
        "     --train_embeddings_dir \"workspace/data//dataperf_{lang}_data/train_embeddings/\" \\\n",
        "     --train_size {TRAIN_SIZE} \\\n",
        "     --outdir \"/drive1/nammt/dataperf-speech-example/dataperf-speech-example\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9SoqEJGg-u_"
      },
      "source": [
        "Finally, let's run a local unofficial evaluation on the results of the training set selection algorithm (`en_25_train.json`, `id_25_train.json`, and `pt_25_train.json`). \n",
        "\n",
        "For each language, we load the coreset training samples specified in the  JSON file, along with evaluation samples specified in `eval.yaml`. We then train an ensemble classifier and [average the macro F1 score across ten random seeds](https://github.com/harvard-edge/dataperf-speech-example/blob/main/eval.py#L139-L154), and display the score (which should match the scores on the DynaBench leaderboard for the coreset sizes of 25 and 60). \n",
        "\n",
        "Here is the expected output for English with a coreset size of 25, using the input of `en_25_train.json` produced by the previous cell:\n",
        "\n",
        "```\n",
        "validating selected IDs\n",
        "loading selected training data\n",
        "Loading targets: 100% 5/5 [00:00<00:00, 17.97it/s]\n",
        "Loading nontargets: 100% 9/9 [00:00<00:00, 140.54it/s]\n",
        "loading eval data\n",
        "Loading targets: 100% 5/5 [00:00<00:00, 119.50it/s]\n",
        "Loading nontargets: 100% 200/200 [00:12<00:00, 16.11it/s]\n",
        "\n",
        "Score:  0.3524448610675314\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validating selected IDs\n",
            "loading selected training data\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 18.79it/s]\n",
            "Loading nontargets: 100%|████████████████████████| 9/9 [00:00<00:00, 102.61it/s]\n",
            "loading eval data\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 75.85it/s]\n",
            "Loading nontargets: 100%|█████████████████████| 200/200 [00:18<00:00, 10.56it/s]\n",
            "Score:  0.31644245062823384\n",
            "validating selected IDs\n",
            "loading selected training data\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 51.58it/s]\n",
            "Loading nontargets: 100%|██████████████████████| 10/10 [00:00<00:00, 195.39it/s]\n",
            "loading eval data\n",
            "Loading targets: 100%|███████████████████████████| 5/5 [00:00<00:00, 143.13it/s]\n",
            "Loading nontargets: 100%|█████████████████████| 200/200 [00:03<00:00, 62.52it/s]\n",
            "Score:  0.36194208552859847\n",
            "validating selected IDs\n",
            "loading selected training data\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 23.42it/s]\n",
            "Loading nontargets: 100%|██████████████████████| 10/10 [00:00<00:00, 127.24it/s]\n",
            "loading eval data\n",
            "Loading targets: 100%|████████████████████████████| 5/5 [00:00<00:00, 75.18it/s]\n",
            "Loading nontargets: 100%|█████████████████████| 200/200 [00:09<00:00, 20.52it/s]\n",
            "Score:  0.4229489086891056\n"
          ]
        }
      ],
      "source": [
        "for lang in [\"en\", \"id\", \"pt\"]:\n",
        "  !python eval.py \\\n",
        "    --language \"{lang}\" \\\n",
        "    --eval_embeddings_dir \"workspace/data/dataperf_{lang}_data/eval_embeddings/\" \\\n",
        "    --train_embeddings_dir \"workspace/data/dataperf_{lang}_data/train_embeddings/\" \\\n",
        "    --allowed_training_set \"workspace/data/dataperf_{lang}_data/allowed_training_set.yaml\" \\\n",
        "    --eval_file \"workspace/data/dataperf_{lang}_data/eval.yaml\" \\\n",
        "    --train_file \"{lang}_{TRAIN_SIZE}_train.json\" \\\n",
        "    --train_size {TRAIN_SIZE} \\\n",
        "    --config_file workspace/dataperf_speech_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
